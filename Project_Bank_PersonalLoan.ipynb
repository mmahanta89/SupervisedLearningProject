{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir():\n",
    "    print(f.ljust(30) +\"--\" + str(round(os.path.getsize(f) / 1000000, 2)) + 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Bank_Personal_Loan_Modelling.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data set has 5000 rows with 14 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All Data types are int/float - 13 columns\n",
    "- Categorical col: ID, ZIP Code, Family, Education, Securities Account, CD Account, Online,CreditCard\n",
    "- Numerical Coulmns: Age, Experience, Income, CCAvg, Mortgage\n",
    "- Dependent Variable/Target Column: Personal Loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ID: all unique values in incremental steps. Customer ID's.\n",
    "- Age: range from 23-69, mean and avg were close around 45. 25%,50% and 75% shows steady increase in value suggests flat like distribution.\n",
    "- Experience: Min value appeared as negative which needs deeper look and cleaning as it should not be less than 0. \n",
    "- Income: range from 8-224K, Outliers towards higher income range. Mean is higher than 50 percentile suggests skewness\n",
    "- Zipcode/Family/Securities Account/CD Account/Online/CreditCard: categorical values. These values looks good and no cleaning is required as per available details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experience: min values appeared as negative, need more analysis on that as Experience should be not be below 0. Values are spreaded and multiple peaks are present.\n",
    "- Age: Values are spread across ages, multiple peaks appears in the distribution.\n",
    "- Income: values are skewed. positive skewness. Lot of outliers towrds higher income.\n",
    "- Family: Distribution is almost similar but family sizes with 1 or 2 are comparatively more in distribution than having 3 or 4. \n",
    "- CCAvg: distribution is skewed (positive).Similar like Income.\n",
    "- Education: Presence of Undergrad is more in the distribution\n",
    "- Mortgage: Skewed data(positive). Majority have mortgage of less value\n",
    "- Personal Loan/Securities Account/CD Account: Less persons have opted for these options\n",
    "- Online: more than 59% cust have opted of online\n",
    "- CreditCard: close to 30% customer has creditcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Experience']<0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Experience']<0].Experience.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All the negative values are for younger populatin age from 23-29 \n",
    "- All are majorly Undergrads in Education\n",
    "- Total 52 records are impacted\n",
    "- Total 3 negative values -1,-2,-3\n",
    "- Need to clean the column\n",
    "- from Profile report we can see Experience has high correlation with Age, need in depth analysis for correlation and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "print('Experience and Age shows very high correlation, will try to replace negative value from same age grop in the fields')\n",
    "print('Experience has relation with Education as well but we dont see much on heatmap.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['Experience']== -1].Age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Experience']== -1].Education.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Experience']== -2].Age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Experience']== -3].Age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['Age']== 23].Experience.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['Age']== 23].Education.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['Age']== 24].Experience.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['Age']== 24].Education.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For age 23 & 24 it seems all values are negative, we can replace Experience to 0 for age 23 & 24 considering the only nearest positive value they got up to is 0.\n",
    "- Considering the education wont affect the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['Age']== 23),'Experience']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['Age']== 24),'Experience']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Experience']== -2].Age.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experience -3 is gone and for -2 only age 25 and 28 are left. we will get mean/median of positive values to replace these ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Experience']== -2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df['Age']== 25) & (df['Education']== 3) & (df['Experience']> -1)].Experience.describe())\n",
    "print('\\nWe will replace the values with median of 0 for age 25, considering the age and eduction of 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[((df['Age']== 25) & (df['Education']== 3) & (df['Experience']< 0)),'Experience']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df['Age']== 28) & (df['Education']== 3) & (df['Experience']> -1)].Experience.describe())\n",
    "print('\\nWe will replace the values with median of 0 for age 28, considering the age and eduction of 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[((df['Age']== 28) & (df['Education']== 3) & (df['Experience']== -2)),'Experience']=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Experience']== -2].Age.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- negative values are gone for -3 and -2, and only -1 is left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Experience']== -1].Age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Experience']== -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df['Age']== 25) & (df['Education']==1) & (df['Experience']> -1)].Experience.describe())\n",
    "print(df[(df['Age']== 25) & (df['Education']==2) & (df['Experience']> -1)].Experience.describe())\n",
    "print('We will replace the values with 50% of 1 for age 25 and edu - 2,1')\n",
    "print(df[(df['Age']== 26) & (df['Education']== 2) & (df['Experience']> -1)].Experience.describe())\n",
    "print('We will replace the values with median of 1 for age 26 and edu - 2')\n",
    "print(df[(df['Age']== 29) & (df['Education']== 3) & (df['Experience']> -1)].Experience.describe())\n",
    "print('We will replace the values with median of 3 for age 29 and edu 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[((df['Age']== 29) & (df['Experience']< 0)),'Experience']=3\n",
    "df.loc[((df['Age']== 26) & (df['Experience']< 0)),'Experience']=1\n",
    "df.loc[((df['Age']== 25) & (df['Experience']< 0)),'Experience']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Experience']<0].Experience.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No more negative values left for Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No Null/Na values appears in the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Columns with Personal Loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,hue='Personal Loan',diag_kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select=df.drop(['ID','ZIP Code','Securities Account','CD Account','Online','CreditCard'],axis=1)\n",
    "#dropping few cols to get a better picture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"with first glance can clearly notice high correlation with age and experience, income and CCAvg, mortgage with Income\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select=df_select.astype({\"Personal Loan\":str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_select,hue=\"Personal Loan\")\n",
    "plt.show()\n",
    "print('''For Age/Experience, Loan distribution looks like evenly distributed,\n",
    "For Income, shows high presence on loan acceptance for high income group\n",
    "For Family, have comparativly higer presne of loan acceptance in family members with 3 or 4\n",
    "For CCAvg,  higer CCAvg group members have higher chances of acceptance of Loan\n",
    "For Education, Loan acceptance grew with education level\n",
    "For Mortgage, doenst provide much data from this view''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Personal Loan'].value_counts())\n",
    "print('Persons taking loan are: 9.6% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID has no relation with anything hence skipping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What age group is taking loan\n",
    "print(df[df['Personal Loan']==1].Age.describe())\n",
    "print('''\\nAge looks distributed across the people taking personal loan.\n",
    "      Majority of cutomers are 35-65 considering 25%.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution of the age group\n",
    "sns.distplot(df[df['Personal Loan']==1].Age,bins=10,color='r',rug=True)\n",
    "sns.distplot(df[df['Personal Loan']==0].Age,bins=10,color='g',rug=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution of the age group\n",
    "#have to make kde false else for the kde representaion its showing both graphs on same scaled level\n",
    "sns.distplot(df[df['Personal Loan']==0]['Age'],bins=10,color='g',kde=False,rug=True);\n",
    "sns.distplot(df[df['Personal Loan']==1]['Age'],bins=10,color='r',kde=False,rug=True);\n",
    "\n",
    "plt.show();\n",
    "print(''' Personal Loan customers looks distributed across the age group''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['Personal Loan']==1].Experience.describe())\n",
    "print('''\\nAge and Experience are highly correlated and expecting same behaviour with Loan as well.\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(df[df['Personal Loan']==1].Experience,bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[df['Personal Loan']==0]['Experience'],bins=10,label='Loan 0',kde=False,rug=True);\n",
    "sns.distplot(df[df['Personal Loan']==1]['Experience'],bins=10,label='Loan 1',kde=False,rug=True);\n",
    "\n",
    "plt.legend();\n",
    "plt.show();\n",
    "\n",
    "print(''' Personal Loan customers looks distributed across the experience range as in age.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['Personal Loan']==1].Income.describe())\n",
    "print(df[df['Personal Loan']==0].Income.describe())\n",
    "print('''\\n Income for cust taken personal loan are ranging from 60-203. For others its ranging from 8-224.\n",
    "This details suggests the personal loan is usually not accepted by income group below 60k \n",
    "and considering the 25% value of 122K, there is higher chance of accepatnce of \n",
    "Personal loan for income group above 122K. \n",
    "\n",
    "      ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[df['Personal Loan']==0]['Income'],kde=False,label='Loan 0',rug=True);\n",
    "sns.distplot(df[df['Personal Loan']==1]['Income'],kde=False,label='Loan 1',rug=True);\n",
    "\n",
    "plt.legend();\n",
    "plt.show();\n",
    "print('''from this image we can assume the personal loan has more cahnce of acceptance towards higher income group.\n",
    "From above details as well we can notice the distribution of customers taking loan is higher above 122K income.\n",
    "The extream right and left of graph suggests these grousp are not so inclienced towards the loan.\n",
    "There is a very high rate of conversion for income group at range from 150-200.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['Personal Loan']==1].Family.describe())\n",
    "print(df[df['Personal Loan']==0].Family.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=df,x='Family',hue='Personal Loan')\n",
    "plt.show();\n",
    "print('''\n",
    "Personal Loan customer looks distributed in all range of family but family memeber with 3 and 4 have higher presence compared to others.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CCAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['Personal Loan']==1].CCAvg.describe())\n",
    "print(df[df['Personal Loan']==0].CCAvg.describe())\n",
    "\n",
    "print('''\n",
    "Personal Loan cust are distributed but cust having higher CCAvg (>8.8) have higer acceptance towards personal Loan.\n",
    "Cust not taking Loan have a max of 8.8 compared to 10 in cust taking Loan.\n",
    "Cust having CCAvg ranging from 2.6-5.35 makes 50% of customers taking loan.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df[df['Personal Loan']==0]['CCAvg'],kde=False,label='Loan 0',rug=True);\n",
    "sns.distplot(df[df['Personal Loan']==1]['CCAvg'],kde=False,label='Loan 1',rug=True);\n",
    "\n",
    "plt.legend();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['Personal Loan']==1].Education.describe())\n",
    "print(df[df['Personal Loan']==0].Education.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df,x='Education',y='Income')\n",
    "print('Mean average seems slightly higher for Undergrads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df,x='Education',y='Income',hue='Personal Loan')\n",
    "print('Personal Loan is more choosen by education group 2 and 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df,x='Education',hue='Personal Loan')\n",
    "plt.show();\n",
    "print('''\n",
    "Personal Loan customer presence is more in education group 3 and 2 compared to 1.\n",
    "There is higher chance of conversion rate if the cust is graduate/Advanced/Professional education level\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mortgage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df[df['Personal Loan']==1].Mortgage.describe())\n",
    "print(df[df['Personal Loan']==0].Mortgage.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.distplot(df[df['Personal Loan']==1]['Mortgage'],kde=False,label='Loan 1');\n",
    "sns.distplot(df[df['Personal Loan']==0]['Mortgage'],kde=False,label='Loan 0');\n",
    "\n",
    "plt.legend();\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminating the mortgage 0 as its making the view very difficult to read\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.distplot(df[(df['Personal Loan']==0) & (df['Mortgage']>0) ]['Mortgage'],kde=False,label='Loan 0',rug=True);\n",
    "sns.distplot(df[(df['Personal Loan']==1) & (df['Mortgage']>0) ]['Mortgage'],kde=False,label='Loan 1',rug=True);\n",
    "\n",
    "\n",
    "plt.legend();\n",
    "plt.show();\n",
    "print('''\n",
    "Higher chance of conversion to Loan for cust having Mortgage above 280/300K\n",
    "(from 50% in below table)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[(df['Personal Loan']==1) & (df['Mortgage']>0)].Mortgage.describe())\n",
    "print(df[(df['Personal Loan']==0) & (df['Mortgage']>0)].Mortgage.describe())\n",
    "print('''\n",
    "Customers having no Mortage have very likelihood of taking PErsonal Loan.\n",
    "For other groups of having varying range of mortages have mostly steady distribution.\n",
    "Comparatively customers having mortgages having more than approx. 280K have higher chances\n",
    "of accepting personal loan (may be because of liqiudity crunch due to higher mortage)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Securities Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df[df['Personal Loan']==1]['Securities Account'].describe())\n",
    "print(df[df['Personal Loan']==0]['Securities Account'].describe())\n",
    "print(\"No relation can be inferred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df,x='Securities Account',y='Income',hue='Personal Loan')\n",
    "print(\"Distribution looks even in both groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Securities Account'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Personal Loan']==1]['Securities Account'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Personal Loan']==0]['Securities Account'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of conversin for people having sec account - '+str((60/522)*100))\n",
    "print('Percentage of conversin for people not having sec account - '+str((420/4478)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df,x='Securities Account',hue='Personal Loan');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.countplot(data=df,x='Securities Account',hue='Personal Loan')\n",
    "total = float(len(df))\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}'.format(height/total),\n",
    "            ha=\"center\")\n",
    "#over all percentage on top of bars #code referenced from https://stackoverflow.com/questions/31749448/how-to-add-percentages-on-top-of-bars-in-seaborn\n",
    "plt.show();\n",
    "print('''\n",
    "Customers not having securities loan have higher conversion but the number of \n",
    "cust not having Sec Account is also higher.\n",
    "Not able to gather much relation from the distribution on securities account''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code referenced from https://stackoverflow.com/questions/31749448/how-to-add-percentages-on-top-of-bars-in-seaborn\n",
    "def with_hue(plot, feature, Number_of_categories, hue_categories):\n",
    "    a = [p.get_height() for p in plot.patches]\n",
    "    patch = [p for p in plot.patches]\n",
    "    for i in range(Number_of_categories):\n",
    "        total = feature.value_counts().values[i]\n",
    "        for j in range(hue_categories):\n",
    "            percentage = '{:.1f}%'.format(100 * a[(j*Number_of_categories + i)]/total)\n",
    "            x = patch[(j*Number_of_categories + i)].get_x() + patch[(j*Number_of_categories + i)].get_width() / 2 - 0.15\n",
    "            y = patch[(j*Number_of_categories + i)].get_y() + patch[(j*Number_of_categories + i)].get_height() \n",
    "            ax.annotate(percentage, (x, y), size = 12)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.countplot(data=df,x='Securities Account',hue='Personal Loan')\n",
    "with_hue(ax,df['Securities Account'],2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Securities Account'],df['Personal Loan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Statiscally people having securietes account have more chances of conversion.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CD Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['CD Account'],df['Personal Loan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.countplot(data=df,x='CD Account',hue='Personal Loan')\n",
    "with_hue(ax,df['CD Account'],2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of conversin for people having CD account - '+str((140/(140+162))*100))\n",
    "print('Percentage of conversin for people not having CD account - '+str((340/(4358+340))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Visually & Statiscally people having CD account have higher chances of conversion for Loan.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Online'],df['Personal Loan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.countplot(data=df,x='Online',hue='Personal Loan')\n",
    "#with_hue(ax,df['Online'],2,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of conversin for people having online account - '+str((291/(2693+291))*100))\n",
    "print('Percentage of conversin for people not having online account - '+str((189/(1827+189))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Online doesnt show a impact on the loan acceptance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['CreditCard'],df['Personal Loan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.countplot(data=df,x='CreditCard',hue='Personal Loan')\n",
    "with_hue(ax,df['CreditCard'],2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of conversin for people having CC - '+str((143/(143+1327))*100))\n",
    "print('Percentage of conversin for people not having CC - '+str((337/(337+3193))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No impact of having CC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(df['Personal Loan']).info())\n",
    "print()\n",
    "print(df['Personal Loan'].value_counts())\n",
    "print('''\\n\n",
    "Total 5000 records out of which only 480 customers have accepted the Loan.\n",
    "''')\n",
    "print(\"\\nOut of total \" + str(df['Personal Loan'].count()) + \", percentage customers opted for loan are: \"+ str(df['Personal Loan'].value_counts(1)[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Personal Loan'].value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Personal Loan'].value_counts())\n",
    "print(\"\\nOut of total \" + str(df['Personal Loan'].count()) + \", percentage customers opted for loan are: \"+ str(df['Personal Loan'].value_counts(1)[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=df,x='Personal Loan')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graphs credit - Referenced from https://www.datacamp.com/community/tutorials/categorical-data\n",
    "labels = df['Personal Loan'].astype('category').cat.categories.tolist()\n",
    "counts = df['Personal Loan'].value_counts()\n",
    "sizes = [counts[var_cat] for var_cat in labels]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\n",
    "ax1.axis('equal')\n",
    "plt.show()\n",
    "print('''There is a huge gap between customers accepting the Loan and total customers,\n",
    "class distribution is severely skewed.\n",
    "This may results in models that have poor predictive performance, \n",
    "specifically for the minority class.''')\n",
    "#https://www.datacamp.com/community/tutorials/diving-deep-imbalanced-data?utm_source=adwords_ppc&utm_campaignid=1455363063&utm_adgroupid=65083631748&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034358&utm_targetid=aud-392016246653:dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=9061996&gclid=Cj0KCQjwvvj5BRDkARIsAGD9vlKQoq5lRfZhmcZAwNSvWsmJM1EIepUab4d5F2WH24kIiOE2Gt7oA3QaApJ2EALw_wcB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.heatmap(df.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()['Personal Loan'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''\n",
    "As analyzed above, Income, CCAvg has highest linear correlation with Loan.\n",
    "Age and Experience are highly correlated but Experience is more negatively correlated with Loan then Age.\n",
    "ZIP Code has least correlation with target along with CreditCard and Online\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and test set in the ratio of 70:30 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing with majority data with no changes, except ID and Age (Age and Experience are highly correlated and dropping one will reduce the complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(['ID','Age','Personal Loan'],axis=1)\n",
    "y=df['Personal Loan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train and Test were splitted into 70:30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train and Test data sets have similar distribution of target variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use different classification models (Logistic, K-NN and NaÃ¯ve Bayes) to predict the likelihood of a customer buying personal loans\n",
    "#Print the confusion matrix for all the above models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logRegModel=LogisticRegression()\n",
    "logRegModel.fit(X_train,y_train)\n",
    "y_predict=logRegModel.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,recall_score,f1_score,precision_score,roc_curve,log_loss,auc\n",
    "print('Accuracy score:',accuracy_score(y_test,y_predict))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_predict))\n",
    "print('Recall Score: ',recall_score(y_test, y_predict))\n",
    "print('Precission Score: ',precision_score(y_test, y_predict))\n",
    "print('F1 Score: ',f1_score(y_test, y_predict))\n",
    "\n",
    "modelComp=pd.DataFrame({'Model':['Logistic Regression - 0.5'],'Accuracy':[accuracy_score(y_test,y_predict)*100],'Precission':[precision_score(y_test, y_predict)*100],'Recall':[recall_score(y_test, y_predict)*100]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression with default thresold - Recall - 33% and Accuracy - 91.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#referenced from training materials\n",
    "from sklearn import metrics\n",
    "def draw_cm( actual, predicted ):\n",
    "    cm = metrics.confusion_matrix( actual, predicted, [0,1] )\n",
    "    sns.heatmap(cm, annot=True,  fmt='.0f', xticklabels = [\"Loan 0\", \"Loan 1\"] , yticklabels = [\"Loan 0\", \"Loan 1\"] )\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_cm(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "#changing the threshold to 0.3\n",
    "y_pred_class = binarize([logRegModel.predict_proba(X_test)[:, 1]], 0.3)[0]\n",
    "\n",
    "print('Accuracy score:',accuracy_score(y_test,y_pred_class))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_pred_class))\n",
    "print('Recall Score: ',recall_score(y_test, y_pred_class))\n",
    "print('Precission Score: ',precision_score(y_test, y_pred_class))\n",
    "print('F1 Score: ',f1_score(y_test, y_pred_class))\n",
    "draw_cm(y_test, y_pred_class)\n",
    "modelComp=modelComp.append(pd.DataFrame({'Model':['Logistic Regression - 0.3'],'Accuracy':[accuracy_score(y_test,y_pred_class)*100],'Precission':[precision_score(y_test, y_pred_class)*100],'Recall':[recall_score(y_test, y_pred_class)*100]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression with threshold decrease to .3 Recall increased to 53.7% and Accuracy - 89.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#referenced from training materials\n",
    "y_pred_proba = logRegModel.predict_proba(X_test)[:, 1]\n",
    "[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\n",
    "plt.ylabel('True Positive Rate (recall)', fontsize=14)\n",
    "plt.title('Receiver operating characteristic (ROC) curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression:\n",
    "- We observe the over all accuracy is hovering around 90% for both threshold levels but recall rate of 33% is very poor.\n",
    "- For predicting customer who could be converted towards taking personal Loan we have to focus more on Recall then Preceission/Accuracy as we dont want to miss any customers who could take a Loan with some compromixe on overall accuracy/precission.\n",
    "- Post dropping the threshold from default to 0.3 we can observe the recall rate increased from 33% to 54%. Still this means we are almost missing half of potential customers. We have almost missed 69 out of 149 total potential customers.\n",
    "- Accuracy also got compramised but its just dropped from 91% to 89% which is acceptable compared to % increase in Recall.\n",
    "- Precission is hovering around 50%-60% which is also not a good sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stats Model Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "logit = sm.Logit( y_train, sm.add_constant( X_train ) )\n",
    "lg = logit.fit()\n",
    "#lg.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=pd.DataFrame(lg.predict( sm.add_constant( X_test ) ))\n",
    "#print(y_predict[0:5])\n",
    "#y_predict.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#referenced from traing materials\n",
    "def get_predictions( y_test, X_test,model ):\n",
    "    y_pred_df = pd.DataFrame( { 'actual': y_test,\n",
    "                               \"predicted_prob\": model.predict( sm.add_constant( X_test ) ) } )\n",
    "    return y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = get_predictions(y_test,X_test, lg )\n",
    "y_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df['predicted'] = y_pred_df.predicted_prob.map( lambda x: 1 if x > 0.3 else 0)\n",
    "y_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=y_pred_df['predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_predict = y_predict.apply( lambda x: 1 if x > 0.6 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy score:',accuracy_score(y_test,y_predict))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_predict))\n",
    "print('Recall Score: ',recall_score(y_test, y_predict))\n",
    "print('Precission Score: ',precision_score(y_test, y_predict))\n",
    "print('F1 Score: ',f1_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logit Recall - 72.5% and Accuracy - 94.07%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_cm(y_test, y_predict)\n",
    "modelComp=modelComp.append(pd.DataFrame({'Model':['Logit -StatsModel - 0.3'],'Accuracy':[accuracy_score(y_test,y_predict)*100],'Precission':[precision_score(y_test, y_predict)*100],'Recall':[recall_score(y_test, y_predict)*100]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = y_pred_df.predicted_prob\n",
    "[fpr1, tpr1, thr1] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr, color='coral', label='ROC curve LogReg (area = %0.2f)' % auc(fpr, tpr))\n",
    "plt.plot(fpr1, tpr1, color='c', label='ROC curve Logit (area = %0.2f)' % auc(fpr1, tpr1))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiverrating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logit StatsModel:\n",
    "- We observe the over all accuracy is around 94% and Recall - 72.5%\n",
    "- Keeping the threshold at 0.3 (Considered from previous run)\n",
    "- Comparing the ROC curve also shows it covers more area compared to LogisticRegression with its default settings\n",
    "- Precisiion is hovering around 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KnnModel = KNeighborsClassifier(n_neighbors=3)\n",
    "KnnModel.fit(X_train,y_train)\n",
    "y_predict=KnnModel.predict(X_test)\n",
    "\n",
    "print('Accuracy score:',accuracy_score(y_test,y_predict))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_predict))\n",
    "print('Recall Score: ',recall_score(y_test, y_predict))\n",
    "print('Precission Score: ',precision_score(y_test, y_predict))\n",
    "print('F1 Score: ',f1_score(y_test, y_predict))\n",
    "\n",
    "draw_cm(y_test, y_predict)\n",
    "modelComp=modelComp.append(pd.DataFrame({'Model':['KNN - 3 Neigbours'],'Accuracy':[accuracy_score(y_test,y_predict)*100],'Precission':[precision_score(y_test, y_predict)*100],'Recall':[recall_score(y_test, y_predict)*100]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN recall - 32.9% Accuracy - 90.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = KnnModel.predict_proba(X_test)[:, 1]\n",
    "[fpr2, tpr2, thr2] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr, color='coral', label='ROC curve LogReg (area = %0.2f)' % auc(fpr, tpr))\n",
    "plt.plot(fpr1, tpr1, color='c', label='ROC curve Logit (area = %0.2f)' % auc(fpr1, tpr1))\n",
    "plt.plot(fpr2, tpr2, color='g', label='ROC curve KNN (area = %0.2f)' % auc(fpr2, tpr2))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiverrating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "- Considering KNN with 3 neighbours, Accuracy - 90.5% and recall - 33%\n",
    "- Precission - 54%\n",
    "- this model has very low recall rate, its only able to predict 49 out of 149 potential customers for Loan.\n",
    "- Srea under ROC is as well way low compared to Logistic Regression & Logit\n",
    "- This is slightly expected as KNN being based on distance and as our data is not scaled, we might be getting biases from variables having higher magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive bayes - GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "NBGauModel = GaussianNB()\n",
    "\n",
    "NBGauModel.fit(X_train,y_train)\n",
    "y_predict=NBGauModel.predict(X_test)\n",
    "\n",
    "print('Accuracy score:',accuracy_score(y_test,y_predict))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_predict))\n",
    "print('Recall Score: ',recall_score(y_test, y_predict))\n",
    "print('Precission Score: ',precision_score(y_test, y_predict))\n",
    "print('F1 Score: ',f1_score(y_test, y_predict))\n",
    "draw_cm(y_test, y_predict)\n",
    "modelComp=modelComp.append(pd.DataFrame({'Model':['Naive Bayes - Gaussian'],'Accuracy':[accuracy_score(y_test,y_predict)*100],'Precission':[precision_score(y_test, y_predict)*100],'Recall':[recall_score(y_test, y_predict)*100]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB Gaussian - Recall - 55.7% Accuracy - 88.13%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = NBGauModel.predict_proba(X_test)[:, 1]\n",
    "[fpr3, tpr3, thr3] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr, color='coral', label='ROC curve LogReg (area = %0.2f)' % auc(fpr, tpr))\n",
    "plt.plot(fpr1, tpr1, color='c', label='ROC curve Logit (area = %0.2f)' % auc(fpr1, tpr1))\n",
    "plt.plot(fpr2, tpr2, color='g', label='ROC curve KNN (area = %0.2f)' % auc(fpr2, tpr2))\n",
    "plt.plot(fpr3, tpr3, color='b', label='ROC curve NaiveBayes (area = %0.2f)' % auc(fpr3, tpr3))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiverrating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes - GaussianNB\n",
    "- Accuracy - 88%, Recall - 56%,Precission - 43%\n",
    "- With all default setting as well Recall rate is better then Logistic and KNN\n",
    "- Area under ROC is also comparable to Logistc Regression\n",
    "- Able to predict 83 out of 149 potential customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBBernoulliModel = BernoulliNB()\n",
    "\n",
    "NBBernoulliModel.fit(X_train,y_train)\n",
    "y_predict=NBBernoulliModel.predict(X_test)\n",
    "\n",
    "print('Accuracy score:',accuracy_score(y_test,y_predict))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_predict))\n",
    "print('Recall Score: ',recall_score(y_test, y_predict))\n",
    "print('Precission Score: ',precision_score(y_test, y_predict))\n",
    "print('F1 Score: ',f1_score(y_test, y_predict))\n",
    "draw_cm(y_test, y_predict)\n",
    "modelComp=modelComp.append(pd.DataFrame({'Model':['Naive Bayes - Bernoulli'],'Accuracy':[accuracy_score(y_test,y_predict)*100],'Precission':[precision_score(y_test, y_predict)*100],'Recall':[recall_score(y_test, y_predict)*100]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB Bernoulli  - Recall - 11.4% Accuracy - 88.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_proba = NBBernoulliModel.predict_proba(X_test)[:, 1]\n",
    "[fpr4, tpr4, thr4] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr, color='coral', label='ROC curve LogReg (area = %0.2f)' % auc(fpr, tpr))\n",
    "plt.plot(fpr1, tpr1, color='c', label='ROC curve Logit (area = %0.2f)' % auc(fpr1, tpr1))\n",
    "plt.plot(fpr2, tpr2, color='g', label='ROC curve KNN (area = %0.2f)' % auc(fpr2, tpr2))\n",
    "plt.plot(fpr3, tpr3, color='b', label='ROC curve NaiveBayes (area = %0.2f)' % auc(fpr3, tpr3))\n",
    "plt.plot(fpr4, tpr4, color='r', label='ROC curve Bernoulli (area = %0.2f)' % auc(fpr4, tpr4))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiverrating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "- Accuracy - 88.7% and Recall - 11.4%\n",
    "- This model is very poor in predicting the potential customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give your reasoning on which is the best model in this case and why it performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_proba = NBBernoulliModel.predict_proba(X_test)[:, 1]\n",
    "[fpr4, tpr4, thr4] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr, color='coral', label='ROC curve LogReg (area = %0.2f)' % auc(fpr, tpr))\n",
    "plt.plot(fpr1, tpr1, color='c', label='ROC curve Logit (area = %0.2f)' % auc(fpr1, tpr1))\n",
    "plt.plot(fpr2, tpr2, color='g', label='ROC curve KNN (area = %0.2f)' % auc(fpr2, tpr2))\n",
    "plt.plot(fpr3, tpr3, color='b', label='ROC curve NaiveBayes (area = %0.2f)' % auc(fpr3, tpr3))\n",
    "plt.plot(fpr4, tpr4, color='r', label='ROC curve Bernoulli (area = %0.2f)' % auc(fpr4, tpr4))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiverrating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelComp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Logistic Rgression, KNN and Naive Bayes\n",
    "- From above comparison table details we can clearly notice with default parameters NB-Gaussian is able to perform/predict better compared to KNN or Logistic Regression(default). As per AUC-ROC as well NB and Logistic regression are very close.\n",
    "- Never the less when we change the threshold value of Logistic regression we were able to get the Recall rate near to NB.\n",
    "- Our main goal is to identify potential customers who could be converted for chosing Loan and for that we need a model which can show better recall rate without compromsing Accuracy.\n",
    "- Considering those two factors only NB - Gaussian seems like a better choice among Logistic, KNN and NB.\n",
    "- Naive Bayes assumes that the features are conditionally independent. In our data set also majority of the attributes/columns are independent. We have removed Age which was highly correlated with Experience and rest other variables has very less depndency on each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional theory details on NB and Logistic Regression referenced from - https://medium.com/@sangha_deb/naive-bayes-vs-logistic-regression-a319b07a5d4c#:~:text=Naive%20Bayes%20also%20assumes%20that,will%20be%20a%20better%20classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelComp.drop(modelComp.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Approach to reduce few less significant columns from data set and captuer performance on model (reduce complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X=df.drop(['ID','Age','Personal Loan'],axis=1)\n",
    "X=df.drop(['ID','Personal Loan','Age','CreditCard','Online','ZIP Code'],axis=1)\n",
    "#X=df.drop(['ID','Personal Loan','Age','Online','ZIP Code'],axis=1)\n",
    "y=df['Personal Loan']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logRegModel=LogisticRegression()\n",
    "logRegModel.fit(X_train,y_train)\n",
    "y_predict=logRegModel.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,recall_score,f1_score,precision_score,roc_curve,log_loss,auc\n",
    "print('Accuracy score:',accuracy_score(y_test,y_predict))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_predict))\n",
    "print('Recall Score: ',recall_score(y_test, y_predict))\n",
    "print('Precission Score: ',precision_score(y_test, y_predict))\n",
    "print('F1 Score: ',f1_score(y_test, y_predict))\n",
    "\n",
    "modelComp2=pd.DataFrame({'Model':['Logistic Regression - 0.5'],'Accuracy':[accuracy_score(y_test,y_predict)*100],'Precission':[precision_score(y_test, y_predict)*100],'Recall':[recall_score(y_test, y_predict)*100]})\n",
    "\n",
    "draw_cm(y_test, y_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the threshold to 0.3\n",
    "y_pred_class = binarize([logRegModel.predict_proba(X_test)[:, 1]], 0.3)[0]\n",
    "\n",
    "print('Accuracy score:',accuracy_score(y_test,y_pred_class))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_pred_class))\n",
    "print('Recall Score: ',recall_score(y_test, y_pred_class))\n",
    "print('Precission Score: ',precision_score(y_test, y_pred_class))\n",
    "print('F1 Score: ',f1_score(y_test, y_pred_class))\n",
    "draw_cm(y_test, y_pred_class)\n",
    "modelComp2=modelComp2.append(pd.DataFrame({'Model':['Logistic Regression - 0.3'],'Accuracy':[accuracy_score(y_test,y_pred_class)*100],'Precission':[precision_score(y_test, y_pred_class)*100],'Recall':[recall_score(y_test, y_pred_class)*100]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logRegModel.predict_proba(X_test)[:, 1]\n",
    "[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\n",
    "plt.ylabel('True Positive Rate (recall)', fontsize=14)\n",
    "plt.title('Receiver operating characteristic (ROC) curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KnnModel = KNeighborsClassifier(n_neighbors=3)\n",
    "KnnModel.fit(X_train,y_train)\n",
    "y_predict=KnnModel.predict(X_test)\n",
    "\n",
    "print('Accuracy score:',accuracy_score(y_test,y_predict))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_predict))\n",
    "print('Recall Score: ',recall_score(y_test, y_predict))\n",
    "print('Precission Score: ',precision_score(y_test, y_predict))\n",
    "print('F1 Score: ',f1_score(y_test, y_predict))\n",
    "\n",
    "draw_cm(y_test, y_predict)\n",
    "modelComp2=modelComp2.append(pd.DataFrame({'Model':['KNN - 3 Neigbours'],'Accuracy':[accuracy_score(y_test,y_predict)*100],'Precission':[precision_score(y_test, y_predict)*100],'Recall':[recall_score(y_test, y_predict)*100]}))\n",
    "y_pred_proba = KnnModel.predict_proba(X_test)[:, 1]\n",
    "[fpr2, tpr2, thr2] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr, color='coral', label='ROC curve LogReg (area = %0.2f)' % auc(fpr, tpr))\n",
    "plt.plot(fpr2, tpr2, color='g', label='ROC curve KNN (area = %0.2f)' % auc(fpr2, tpr2))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiverrating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB - Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "NBGauModel = GaussianNB()\n",
    "\n",
    "NBGauModel.fit(X_train,y_train)\n",
    "y_predict=NBGauModel.predict(X_test)\n",
    "\n",
    "print('Accuracy score:',accuracy_score(y_test,y_predict))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_predict))\n",
    "print('Recall Score: ',recall_score(y_test, y_predict))\n",
    "print('Precission Score: ',precision_score(y_test, y_predict))\n",
    "print('F1 Score: ',f1_score(y_test, y_predict))\n",
    "draw_cm(y_test, y_predict)\n",
    "modelComp2=modelComp2.append(pd.DataFrame({'Model':['Naive Bayes - Gaussian'],'Accuracy':[accuracy_score(y_test,y_predict)*100],'Precission':[precision_score(y_test, y_predict)*100],'Recall':[recall_score(y_test, y_predict)*100]}))\n",
    "\n",
    "y_pred_proba = NBGauModel.predict_proba(X_test)[:, 1]\n",
    "[fpr3, tpr3, thr3] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr, color='coral', label='ROC curve LogReg (area = %0.2f)' % auc(fpr, tpr))\n",
    "plt.plot(fpr2, tpr2, color='g', label='ROC curve KNN (area = %0.2f)' % auc(fpr2, tpr2))\n",
    "plt.plot(fpr3, tpr3, color='b', label='ROC curve NaiveBayes (area = %0.2f)' % auc(fpr3, tpr3))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiverrating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelComp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelComp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison - Logistic Regression, KNN, NB\n",
    "- There is a significant improvement in recall rate and accuracy of all the models once we dropped few less significant columns\n",
    "- with current model Logistic regression is able to predict 109 out of 149 potential customers. Logistic Regression will be better choice in this configuration\n",
    "- KNN's performance is also increased but to a very slight level\n",
    "- NB didnt showed any significant improvementat all on Recall rate\n",
    "- With current identified columns we can take Logistic regression with 0.3 threshold as best performing model with Recall rate of 73% and Accuracy - 94%\n",
    "- Logistic regression measures the relationship between a output variable and one or more independent variables, which are usually (but not necessarily) continuous, by using probability scores as the predicted values of the dependent variable. By reducing the complexity/variables in the eqations, model is able to predict better. BUt NB reamins at same level as Naive Bayes figures out how the data was generated given the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional theory details on NB and Logistic Regression referenced from - https://medium.com/@sangha_deb/naive-bayes-vs-logistic-regression-a319b07a5d4c#:~:text=Naive%20Bayes%20also%20assumes%20that,will%20be%20a%20better%20classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next - scaling the data - as values in the columns are on very diff scale then each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logRegModel=LogisticRegression()\n",
    "logRegModel.fit(X_train_scaled,y_train)\n",
    "y_predict=logRegModel.predict(X_test_scaled)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,recall_score,f1_score,precision_score,roc_curve,log_loss,auc\n",
    "print('Accuracy score:',accuracy_score(y_test,y_predict))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_predict))\n",
    "print('Recall Score: ',recall_score(y_test, y_predict))\n",
    "print('Precission Score: ',precision_score(y_test, y_predict))\n",
    "print('F1 Score: ',f1_score(y_test, y_predict))\n",
    "\n",
    "modelComp3=pd.DataFrame({'Model':['Logistic Regression - 0.5'],'Accuracy':[accuracy_score(y_test,y_predict)*100],'Precission':[precision_score(y_test, y_predict)*100],'Recall':[recall_score(y_test, y_predict)*100]})\n",
    "\n",
    "draw_cm(y_test, y_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the threshold to 0.3\n",
    "y_pred_class = binarize([logRegModel.predict_proba(X_test_scaled)[:, 1]], 0.3)[0]\n",
    "\n",
    "print('Accuracy score:',accuracy_score(y_test,y_pred_class))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_pred_class))\n",
    "print('Recall Score: ',recall_score(y_test, y_pred_class))\n",
    "print('Precission Score: ',precision_score(y_test, y_pred_class))\n",
    "print('F1 Score: ',f1_score(y_test, y_pred_class))\n",
    "draw_cm(y_test, y_pred_class)\n",
    "modelComp3=modelComp3.append(pd.DataFrame({'Model':['Logistic Regression - 0.3'],'Accuracy':[accuracy_score(y_test,y_pred_class)*100],'Precission':[precision_score(y_test, y_pred_class)*100],'Recall':[recall_score(y_test, y_pred_class)*100]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logRegModel.predict_proba(X_test_scaled)[:, 1]\n",
    "[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\n",
    "plt.ylabel('True Positive Rate (recall)', fontsize=14)\n",
    "plt.title('Receiver operating characteristic (ROC) curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KnnModel = KNeighborsClassifier(n_neighbors=3)\n",
    "KnnModel.fit(X_train_scaled,y_train)\n",
    "y_predict=KnnModel.predict(X_test_scaled)\n",
    "\n",
    "print('Accuracy score:',accuracy_score(y_test,y_predict))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_predict))\n",
    "print('Recall Score: ',recall_score(y_test, y_predict))\n",
    "print('Precission Score: ',precision_score(y_test, y_predict))\n",
    "print('F1 Score: ',f1_score(y_test, y_predict))\n",
    "\n",
    "draw_cm(y_test, y_predict)\n",
    "modelComp3=modelComp3.append(pd.DataFrame({'Model':['KNN - 3 Neigbours'],'Accuracy':[accuracy_score(y_test,y_predict)*100],'Precission':[precision_score(y_test, y_predict)*100],'Recall':[recall_score(y_test, y_predict)*100]}))\n",
    "y_pred_proba = KnnModel.predict_proba(X_test_scaled)[:, 1]\n",
    "[fpr2, tpr2, thr2] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr, color='coral', label='ROC curve LogReg (area = %0.2f)' % auc(fpr, tpr))\n",
    "plt.plot(fpr2, tpr2, color='g', label='ROC curve KNN (area = %0.2f)' % auc(fpr2, tpr2))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiverrating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB - Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,BernoulliNB\n",
    "NBGauModel = GaussianNB()\n",
    "\n",
    "NBGauModel.fit(X_train_scaled,y_train)\n",
    "y_predict=NBGauModel.predict(X_test_scaled)\n",
    "\n",
    "print('Accuracy score:',accuracy_score(y_test,y_predict))\n",
    "print('confuion matrix:\\n',confusion_matrix(y_test,y_predict))\n",
    "print('Recall Score: ',recall_score(y_test, y_predict))\n",
    "print('Precission Score: ',precision_score(y_test, y_predict))\n",
    "print('F1 Score: ',f1_score(y_test, y_predict))\n",
    "draw_cm(y_test, y_predict)\n",
    "modelComp3=modelComp3.append(pd.DataFrame({'Model':['Naive Bayes - Gaussian'],'Accuracy':[accuracy_score(y_test,y_predict)*100],'Precission':[precision_score(y_test, y_predict)*100],'Recall':[recall_score(y_test, y_predict)*100]}))\n",
    "\n",
    "y_pred_proba = NBGauModel.predict_proba(X_test_scaled)[:, 1]\n",
    "[fpr3, tpr3, thr3] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(fpr, tpr, color='coral', label='ROC curve LogReg (area = %0.2f)' % auc(fpr, tpr))\n",
    "plt.plot(fpr2, tpr2, color='g', label='ROC curve KNN (area = %0.2f)' % auc(fpr2, tpr2))\n",
    "plt.plot(fpr3, tpr3, color='b', label='ROC curve NaiveBayes (area = %0.2f)' % auc(fpr3, tpr3))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiverrating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default\n",
    "modelComp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Removed few columns to reduce complexity\n",
    "modelComp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled data\n",
    "modelComp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison\n",
    "- Noticed Significant improvement in KNN and NB \n",
    "- No improvement noticed for Logistic Regression\n",
    "- Considering the scalled data with reduced columns we can notice the KNN has a significantly high rate for Accuracy, Precission. Recall rate is very close to Logistic Regression with 0.3 threshold.\n",
    "- KNN model is able to predict 102 out of 149 where as Logistic regression with 0.3 threshold predicted 109 out of 149 potential customers.\n",
    "- Considering the Accuracy where KNN only had 6 values misclassifed from Loan 0 to Loan 1 class, compared to 57 in Logistic regression.\n",
    "- Considering above parameters we can suggest KNN would work better if we have scalled data compared to other models.\n",
    "- KNN is a distance based algorithm, it chooses the k closest neighbors and then based on these neighbors, assigns a class  or predicts a value for new observation. Hence it was affected by scale of the variables. If the data is not scaled it will give higher weightage to those variables which has high magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN theory referenced from - https://medium.com/analytics-vidhya/why-is-scaling-required-in-knn-and-k-means-8129e4d88ed7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End OF File"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
